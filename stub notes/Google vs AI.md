
Pro Google
- Google has insane AI expertise
	- Google BERT has been running billions of times a day
		- https://twitter.com/sterlingcrispin/status/1606064294663069696?s=20
	- Google PaLM (Pathways Language Model) in 2021
		- Pathways will enable us to train a single model to do thousands or millions of tasks.
			- We want a model to have different capabilities that can be called upon as needed, and stitched together to perform new, more complex tasks – a bit closer to the way the mammalian brain generalizes across tasks.
		- Pathways will enable multimodal models that encompass vision, auditory, and language understanding simultaneously. 
			- And of course an AI model needn’t be restricted to these familiar senses; Pathways could handle more abstract forms of data, helping find useful patterns that have eluded human scientists in complex systems such as climate dynamics.
		- Pathways will make models sparse and efficient, only activating the relevant parts of the network for the given task.
			- For example, GShard and Switch Transformer are two of the largest machine learning models we’ve ever created, but because both use sparse activation, they [consume less than 1/10th the energy](https://blog.google/technology/ai/minimizing-carbon-footprint/) that you’d expect of similarly sized dense models — while being as accurate as dense models.
		- PaLM 540B beats GPT3 175B on all categories https://twitter.com/sterlingcrispin/status/1606309065730170880/photo/1
	- Google LaMDA
		- https://blog.google/technology/ai/lamda/
- PageRank origins ([tweet](https://twitter.com/mmitchell_ai/status/1605013368560943105?s=20)) 
	- pre pagerank was gpt-retrieval like
	- With PageRank, the fact that websites link to one another could be used to identify which websites were *the most* linked to. The *most linked* sites are the ones people tend to want.
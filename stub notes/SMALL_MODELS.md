## opinions

large models are demoralizing https://www.reddit.com/r/MachineLearning/comments/wiqjxv/d_the_current_and_future_state_of_aiml_is/

## directions


CRAMMING: TRAINING A LANGUAGE MODEL ON A SINGLE GPU IN ONE DAY https://arxiv.org/pdf/2212.14034v1.pdf


light BERT model?
https://twitter.com/deepwhitman/status/1610354790772637697?s=46&t=m_H3h2AbaafHjQJEu1cHJw


https://arxiv.org/abs/2301.00774
Massive Language Models Can Be Accurately Pruned in One-Shot

https://arxiv.org/abs/2202.08906 [**ST-MoE: Designing Stable and Transferable Sparse Expert Models**](https://arxiv.org/abs/2202.08906)

Best sparse model paper. Meticulous experiments, comprehensive evals and the goto reference for all sparse modeling


H3 state space models https://twitter.com/mathemagic1an/status/1617620133182332928?s=46&t=I3QXnpdfF5CXpszIYkv4pQ
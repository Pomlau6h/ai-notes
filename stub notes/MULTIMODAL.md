- What is CLIP? CLIP is the first multimodal (in this case, vision and text) model tackling computer vision https://www.kdnuggets.com/2021/03/beginners-guide-clip-model.html
- DeepMind announced [Gato](https://www.deeplearning.ai/the-batch/one-model-hundreds-of-tasks/), a transformer that It learned over 600 diverse tasks — playing Atari games, stacking blocks using a robot arm, generating image captions, and so on — though not necessarily as well as separate models dedicated to those tasks. The system underwent supervised training on a wide variety of datasets simultaneously, from text and images to actions generated by reinforcement learning agents.
-   As the year drew to a close, researchers at Google brought a similar range of abilities to robotics. [RT-1](https://ai.googleblog.com/2022/12/rt-1-robotics-transformer-for-real.html?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8HbXG-ZkwAj82Nv49uUrBwOHz4zUj3mkyjIfEd5lU7h3JHZR0pEG5OpkUCPPqwWvqMbjWl) is a transformer that enables robots to perform over 700 tasks. The system, which tokenizes actions as well as images, learned from a dataset of 130,000 episodes collected from a fleet of robots over nearly a year and a half. It achieved outstanding zero-shot performance in new tasks, environments, and objects compared to prior techniques.
- Palm E robotics and vision demo https://twitter.com/dannydriess/status/1632904675124035585?s=46&t=90xQ8sGy63D2OtiaoGJuww
- google MUM https://blog.google/products/search/introducing-mum/
- "this is not a pipe" really is a pipe
- Flamingo models take advantage of two complementary  models: a vision model that analyzes visual scenes and a large language model   which performs a basic form of reasoning. The language model is trained on a  large amount of text data. https://arxiv.org/pdf/2301.04655.pdf
	- independent reproductions https://twitter.com/sanhestpasmoi/status/1632775840135016448?s=46&t=90xQ8sGy63D2OtiaoGJuww
	- Openflamingo https://laion.ai/blog/open-flamingo/  an open-source reproduction of DeepMind's Flamingo model. At its core, OpenFlamingo is a framework that enables training and evaluation of large multimodal models (LMMs).
	- The Multimodal-C4 dataset is an expansion of the text-only [C4 dataset](https://www.tensorflow.org/datasets/catalog/c4), which was used to train [T5 models](https://arxiv.org/abs/1910.10683). For each document in the [C4 en.clean](https://www.tensorflow.org/datasets/catalog/c4#c4en_default_config) dataset, we retrieve the original webpage from [Common Crawl](https://commoncrawl.org/), then collect the downloadable images. Data cleaning is carried out through deduplication and content filtering, which aims to eliminate non-safe for work (NSFW) and unrelated images, such as advertisements. Additionally, we run face detection and discard images with positive identifications. Finally, images and sentences are interleaved using bipartite matching within a document: CLIP ViT/L-14 image-text similarities serve as edge weights. Multimodal-C4 consists of approximately 75 million documents, encompassing around 400M images and 38B tokens. 
- MiniGPT-4 https://minigpt-4.github.io/
	- On a technical level, they're doing something really simple -- take BLIP2's ViT-L+Q-former, connect it to Vicuna-13B with a linear layer, and train just the tiny layer on some datasets of image-text pairs.

microsoft
- we introduce a new computer vision foundation model, Florence, to expand the representations from coarse (scene) to fine (object), from static (images) to dynamic (videos), and from RGB to multiple modalities (caption, depth). By incorporating universal visual-language representations from Web-scale image-text data, our Florence model can be easily adapted for various computer vision tasks, such as classification, retrieval, object detection, VQA, image caption, video retrieval and action recognition.

google multimodal researchc https://ai.googleblog.com/2023/01/google-research-2022-beyond-language.html?m=1


visual chatgpt https://github.com/microsoft/visual-chatgpt


hugging gpt - chatgpt coordinating between huggingface models https://arxiv.org/abs/2303.17580

https://text-generator.io/blog/embed-images-text-and-code Embed Images, Text and Code in the same space

https://www.theverge.com/2023/5/9/23716558/meta-imagebind-open-source-multisensory-modal-ai-model-research The new ImageBind model combines text, audio, visual, movement, thermal, and depth data.


Gato model - 1.2b param model

### GPT4 multimodal 
https://twitter.com/DrJimFan/status/1634244545360609289

Shameless plug -- our recent model FROMAGe is one step towards enabling existing LLMs to use images as both inputs and outputs: [https://jykoh.com/fromage](https://t.co/LTEqt2kNvi)

Announcing Genmo Chat, a creative copilot that uses GPT-4 and a large suite of generative AI tools to create and then edit any video or image you ask for. ([tweet](https://twitter.com/genmoai/status/1640760678167478274))



-  LSTM is dead. Long Live Transformers! ([talk](https://www.youtube.com/watch?v=S27pHKBEp30))
	- Â the evolution of natural language processing (NLP) techniques, starting with the limitations of bag-of-words models and vanilla recurrent neural networks (RNNs), which suffer from vanishing and exploding gradients. 
	- The introduction of long short-term memory (LSTM) resolved these issues but was still challenging to train and lacked transfer learning abilities, leading to the development of the transformer model. 
	- The transformer model uses **self-attention** and **a feedforward neural network** to read input sequences and create output sequences, incorporating **multi-headed attention** to generate multiple attention outputs with different sets of parameters. 
	- Key innovations of the transformer model include **positional encoding** and the use of **ReLU activation** functions. 
	- The speaker highlights the advantages of using transformers and models like Roberta for training models on large-scale unsupervised text data, enabling transfer learning and reduced training time and resources.
	- Despite being replaced in most areas by transformers, LSTM still has applications in real-time control. 
	- The speaker also compares word CNN to transformers and observes that transformers can offer contextual answers more efficiently across the entire document.